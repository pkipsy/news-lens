{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Sides Media Spider\n",
    "\n",
    "[All Sides](http://allsides.com/) looks to provide a more balanced approach to news coverage by rounding up the top headlines of the day and showcasing reporting of these stories by outlets on the left, right, and center of the aisle.\n",
    "\n",
    "This spider has been designed to crawl the [All Sides](http://allsides.com/) website to extract information about the last two years of headline roundups. For every roundup, it harvests the All Sides' headline and short summary, as well as information about the linked articles, including - their source, source's bias, source's headline, and url.\n",
    "\n",
    "Each article's url information can then be fed through [NewsPlease](https://github.com/fhamborg/news-please) to extract its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Sides has a running list of all their headline roundups over the years. Here, we begin by identifying the pages that contain headline roundups from the last two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#harvest a seed list of pages to crawl from All Sides Media\n",
    "pages = []\n",
    "\n",
    "def get_seed(n):\n",
    "    '''\n",
    "    n defines the number of pages back to pull\n",
    "    n=56 steps back to June 2016 (as of June 2018)\n",
    "    '''\n",
    "    for i in range(0, n+1):\n",
    "        url = 'https://www.allsides.com/story-list?page=' + str(i)\n",
    "        pages.append(url)\n",
    "\n",
    "get_seed(56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we iterate through these pages, building out our own list of links to all the relevant roundups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up BeautifulSoup to run over All Sides Media\n",
    "link_harvest = []\n",
    "\n",
    "# helper function to harvest and parse pages\n",
    "def soup_basics(item):\n",
    "    page = requests.get(item)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def harvest_links(pages):\n",
    "    '''\n",
    "    runs the parser over submitted pages\n",
    "    identifies headline link content in the extracted page\n",
    "    appends relevant links to a list\n",
    "    '''\n",
    "    for item in pages:\n",
    "        soup = soup_basics(item)\n",
    "\n",
    "        # Pull all headlines from the featured stories under class 'view-content'\n",
    "        story_headline_list = soup.find(class_='view-content')\n",
    "        # Pull headline/link text from all instances of <a> tag\n",
    "        story_list_items = story_headline_list.find_all('a')\n",
    "\n",
    "        #harvest the headline and link information\n",
    "        for story_headline in story_list_items:\n",
    "            #headline = story_headline.contents[0]\n",
    "            #headline = headline.encode(\"utf8\").strip()\n",
    "            link = 'https://www.allsides.com'+story_headline.get('href')\n",
    "            link_harvest.append(link)\n",
    "\n",
    "harvest_links(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function harvests links to all of the news articles listed in All Sides' daily [headline roundup](https://www.allsides.com/story-list). This list can then be run through Boilerpipe's web API for [article extraction](http://boilerpipe-web.appspot.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all news article links\n",
    "all_articles = []\n",
    "\n",
    "def extract_articles(link_harvest):\n",
    "    for link_content in link_harvest:\n",
    "        soup = soup_basics(link_content)\n",
    "\n",
    "        # locate relevant information within the extracted page\n",
    "        substory_list = soup.find_all(class_='news-title')\n",
    "\n",
    "        # loop through the different news sources within each major news story\n",
    "        for i in range(0,len(substory_list)):\n",
    "            substory_items = substory_list[i].find_all('a')\n",
    "            for substory_headline in substory_items:\n",
    "                link = substory_headline.get('href')\n",
    "                all_articles.append(link)\n",
    "\n",
    "extract_articles(link_harvest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save news article links to file\n",
    "link_file = open('link_file.txt', 'w')\n",
    "\n",
    "for item in all_articles:\n",
    "    link_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions harvest detailed content on the news articles listed in All Sides' daily headline roundup and save them to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to encode strings for csv\n",
    "def csv_encoder(text_string):\n",
    "    coded = text_string.encode(\"utf-8\").strip()\n",
    "    return coded\n",
    "\n",
    "# extract all content\n",
    "\n",
    "def extract_content(link_harvest):\n",
    "    '''\n",
    "    for each story, pulls the shared news headline, date, and summary description\n",
    "    for each news source, identifies the source bias (liberal, conservative, center) & outgoing link\n",
    "    uses re and .contents to clean harvested text\n",
    "    writes collected, cleaned data to csv\n",
    "    '''\n",
    "\n",
    "    # open csv file to store info\n",
    "    file = open('allsides-content.csv', 'w')\n",
    "    fileWriter = csv.writer(file, delimiter=' ', quotechar='|')\n",
    "    fileWriter.writerow(['Date', 'AllMedia_Headline', 'Description', 'Source_Name','Source_Bias', 'Source_Headline', 'Source_Link'])\n",
    "\n",
    "    try:\n",
    "        for link_content in link_harvest:\n",
    "            soup = soup_basics(link_content)\n",
    "\n",
    "            # locate relevant information within the extracted page\n",
    "            story_headline = soup.find(class_='taxonomy-heading')\n",
    "            story_date = soup.find(property='dc:date')\n",
    "            story_description = soup.find(class_='story-id-page-description')\n",
    "            substory_source = soup.find_all(class_='news-source')\n",
    "            substory_bias = soup.find_all(class_='global-bias')\n",
    "            substory_list = soup.find_all(class_='news-title')\n",
    "\n",
    "            # loop through the different news sources within each major news story\n",
    "            n=0\n",
    "            for i in range(0,len(substory_list)):\n",
    "                substory_items = substory_list[i].find_all('a')\n",
    "                for substory_headline in substory_items:\n",
    "\n",
    "                    clean_date = story_date.contents[0]\n",
    "\n",
    "                    clean_headline = re.sub('\\W+',' ', story_headline.contents[0])[1:][:-1]\n",
    "                    clean_headline = csv_encoder(clean_headline)\n",
    "\n",
    "                    try:\n",
    "                        clean_description = str(story_description.contents[1])[3:][:-4]\n",
    "                    except IndexError:\n",
    "                        clean_description = 'Null'\n",
    "\n",
    "                    try:\n",
    "                        clean_source = substory_source[n].contents[1].contents[0]\n",
    "                    except IndexError:\n",
    "                        clean_source = 'Unknown'\n",
    "\n",
    "                    clean_bias = substory_bias[n]\n",
    "                    clean_bias = re.sub('\\W+',' ', clean_bias.contents[0])[10:]\n",
    "                    n=n+1\n",
    "\n",
    "                    headline = substory_headline.contents[0]\n",
    "                    headline = csv_encoder(headline)\n",
    "\n",
    "                    link = substory_headline.get('href')\n",
    "\n",
    "                    fileWriter.writerow([clean_date, clean_headline, clean_description, clean_source, clean_bias, headline, link])\n",
    "\n",
    "    except socket.error as err:\n",
    "        print('Socket connection error... Waiting 10 seconds to retry.')\n",
    "        del self.sock\n",
    "        time.sleep(10)\n",
    "        try_count += 1\n",
    "\n",
    "    file.close()\n",
    "\n",
    "extract_content(link_harvest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
