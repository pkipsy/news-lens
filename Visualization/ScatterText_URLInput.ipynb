{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scattertext Visualization of URL Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install news-please\n",
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "!pip install scattertext\n",
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports url transformer to analyze input url and get a recommended article\n",
    "from sandbox import InputModel, OutputModel, News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import scattertext modules\n",
    "%matplotlib inline\n",
    "import scattertext as st\n",
    "import re, io\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata, hmean, norm\n",
    "import spacy\n",
    "import os, pkgutil, json, urllib\n",
    "from urllib.request import urlopen\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "from scattertext import CorpusFromPandas, produce_scattertext_explorer\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import text processing modules\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Viz(News):\n",
    "  \n",
    "    def __init__(self, url):\n",
    "        News.__init__(self, url)\n",
    "\n",
    "    def get_texts(self):\n",
    "      ''' \n",
    "      get text of input url & first recommended article\n",
    "      write to dataframe with bias information\n",
    "      '''\n",
    "  \n",
    "      # load text of user input\n",
    "      user_input = self.article.text\n",
    "      user_input = user_input.split('\\n')\n",
    "  \n",
    "      # load text of top recommended article\n",
    "      rec_article = self.choose_news()\n",
    "      rec_url = rec_article['url'][0]\n",
    "      rec_text = OutputModel(rec_url).article.text\n",
    "      rec_text = rec_text.split('\\n')\n",
    "  \n",
    "      # create dataframe\n",
    "      text = user_input + rec_text\n",
    "      bias = ['right']*len(user_input)+['left']*len(rec_text)\n",
    "      position = list(range(0,len(text)))\n",
    "      data = {'bias':bias, 'position':position, 'text':text}\n",
    "      df = pd.DataFrame(data, columns = ['bias', 'position', 'text'])\n",
    "      \n",
    "      # shuffle df for random sampling\n",
    "      df = df.sample(frac=1).reset_index(drop=True)\n",
    "      \n",
    "      return df\n",
    "    \n",
    "    def clean_texts(self):\n",
    "      ''' text preprocessing '''\n",
    "      \n",
    "      df = self.get_texts()\n",
    "      \n",
    "      # rewrite df with cleaned text\n",
    "      for i in range(0, len(df)):\n",
    "        text = df.at[i,'text']\n",
    "      \n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n',' ')\n",
    "\n",
    "        numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "        banned = [\"’\",\"’\",\"“\",\"—\",\"”\",\"‘\",\"–\",'#','[','/','(',')','{','}','\\\\','[',']','|','@',',',';','+','-']\n",
    "        banned = ''.join(banned) + ''.join(numbers)\n",
    "        banned = banned.replace(\".\", \"\")\n",
    "        stop_list = set(stopwords.words('english'))\n",
    "\n",
    "        translation_table = dict.fromkeys(map(ord, banned), ' ')\n",
    "        text = text.translate(translation_table)\n",
    "        text = re.sub(' +',' ',text)\n",
    "        text = ' '.join([word for word in text.split() if word not in stop_list])\n",
    "        \n",
    "        df.at[i,'text'] = text\n",
    "      \n",
    "      return df\n",
    "    \n",
    "    def create_corpus(self):\n",
    "      \n",
    "      # load cleaned df\n",
    "      convention_df = self.clean_texts()\n",
    "      \n",
    "      # create parsed corpus\n",
    "      convention_df.groupby('bias').apply(lambda x: x.text.apply(lambda x: len(x.split())).sum())\n",
    "      convention_df['parsed'] = convention_df.text.apply(nlp)\n",
    "      corpus = st.CorpusFromParsedDocuments(convention_df, category_col='bias', parsed_col='parsed').build()\n",
    "      \n",
    "      # remove stop words\n",
    "      stop_word_list = ['via getty', 'inbox', 'subscribe', '×', 'close ×', 'screen close', 'full screen', 'buy second', 'second continue', 'story continued', 'llc permission', '―', 'xe', '\\\\xe2\\\\x80\\\\x99', 'news', 'for reprint', 'llc', 'post', 'click', 'to', '’ve', 'unsupported on', 'share', 'that ’s', 'still', 'got', 'it', '37', 'of his', 'this report', 'ofs', 'fox', 'photos', '’m', 'is the', 's.', 'around', 'times', 'also', 'the', 'copyright', 'washington times', 'mr', 'press', 'wait', 'associated', 'unsubscribe', 'view', 'photo wait', 'http', '#', 'associated press', 'more videos', 'get', 'just watched', 'permission', 'however', 'b.', 'ms.', 'here©', 'device', 'copyright ©', 'paste', '10', 'the associated', 'contributed to', 'hide', 'and his', 'videos', 'said mr.', '_', '©', 'contributed', 'embed', 'n’t', '/', 'something', 'i', 'that they', 'read', 'for a', 'playback', 'must watch', 'washington post', 'just', 'to get', 'r', 'read more', 'toggle', 'more', 'i ’m', 'follow', 'is', 'https', ' ', 'said', 'mr.', 'unsupported', 'or blog', 'your device', 'for', 'cnn', 'of 76', 'that', 'ms', 'andhis', 'click here', 'or share', 'replay', 'press contributed', 'they', 'must', 'prof', 'www', 'it ’s', 'told', '’re', 'the washington', '1', \"'s rise\", '© 2018', 'to this', 'skip', 'around the', 'blog', 'cut', 'told fox', 'mrs.', 'hide caption', 'ad', 'watched', '/ the', 'replay more', 'and the', '’s', '2018', 'copy', '&', 'read or', 'reprint permission', 'are', 'told cnn', 'watch', 'here for', 'also said', 'copy this', 'reprint', 'report', 'advertisement', 'mrs', 'caption', 'autoplay', 'fox news', 'dr', 'enlarge', 'times llc', '76', 'photo', 'this']\n",
    "      stop_word_list = list(set(stop_word_list))\n",
    "\n",
    "      update_stop = []\n",
    "      for term in stop_word_list:\n",
    "        if term in corpus._term_idx_store:\n",
    "          update_stop.append(term)\n",
    "      corpus = corpus.remove_terms(update_stop)\n",
    "      \n",
    "      return corpus\n",
    "    \n",
    "    def scatter_viz(self):\n",
    "      \n",
    "      #load corpus\n",
    "      corpus = self.create_corpus()\n",
    "      \n",
    "      html = produce_scattertext_explorer(corpus,\n",
    "                                    category='left',\n",
    "                                    category_name='Democratic',\n",
    "                                    not_category_name='Republican',\n",
    "                                    width_in_pixels=1000,\n",
    "                                    minimum_term_frequency=1,\n",
    "                                    metadata=convention_df['position'],\n",
    "                                    term_significance = st.LogOddsRatioUninformativeDirichletPrior())\n",
    "\n",
    "      file_name = 'Example_Scattertext_RankDefault.html'\n",
    "      open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "      \n",
    "      return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Live URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tester2 = Viz('http://www.foxnews.com/politics/2018/07/05/trump-claims-maxine-waters-iq-in-mid-60s-slams-fake-pocahontas-elizabeth-warren-in-rally-to-unseat-jon-tester.html').scatter_viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"700\"\n",
       "            src=\"Example_Scattertext_RankDefault.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f982f3ad9b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src=tester2, width = 1200, height=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
