{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Left</td>\n",
       "      <td>Jared Bernstein, a former chief economist to V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Right</td>\n",
       "      <td>Liberals have opposed virtually every move Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Center</td>\n",
       "      <td>CLOSE President Trump’s once bitter political ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Center</td>\n",
       "      <td>The attorneys for Michael Cohen, President Don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Left</td>\n",
       "      <td>Longtime Trump lawyer Michael Cohen is changin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bias                                               text\n",
       "0    Left   Jared Bernstein, a former chief economist to V...\n",
       "1   Right   Liberals have opposed virtually every move Pre...\n",
       "2  Center   CLOSE President Trump’s once bitter political ...\n",
       "3  Center   The attorneys for Michael Cohen, President Don...\n",
       "4    Left   Longtime Trump lawyer Michael Cohen is changin..."
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in file from csv\n",
    "df = pd.read_csv('news-corpus-df.csv')\n",
    "\n",
    "# drop articles from df that have less than 200 words of text\n",
    "new_df = df.drop(df[df.text_len < 250].index)\n",
    "\n",
    "# subset source bias, and article text\n",
    "new_df = new_df.loc[:, ['bias', 'text']]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Prep for fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Left</td>\n",
       "      <td>Jared Bernstein, a former chief economist to V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Right</td>\n",
       "      <td>Liberals have opposed virtually every move Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Center</td>\n",
       "      <td>CLOSE President Trump’s once bitter political ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Center</td>\n",
       "      <td>The attorneys for Michael Cohen, President Don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Left</td>\n",
       "      <td>Longtime Trump lawyer Michael Cohen is changin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bias                                               text\n",
       "0    Left  Jared Bernstein, a former chief economist to V...\n",
       "1   Right  Liberals have opposed virtually every move Pre...\n",
       "2  Center  CLOSE President Trump’s once bitter political ...\n",
       "3  Center  The attorneys for Michael Cohen, President Don...\n",
       "4    Left  Longtime Trump lawyer Michael Cohen is changin..."
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare to write txt file (only for fastText)\n",
    "new_df = new_df.replace('Left ', 'Left')\n",
    "new_df = new_df.replace('Right ', 'Right')\n",
    "new_df = new_df.replace('Center ', 'Center')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3162 articles and 14233550 words in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# get the shape of the df\n",
    "num_rows = new_df.shape[0]\n",
    "\n",
    "# get the number of words in the corpus\n",
    "corpus_size = 0\n",
    "for i in range(0, num_rows):\n",
    "    corpus_size += len(new_df.iloc[i][1])\n",
    "\n",
    "print('There are '+str(num_rows)+' articles and '+str(corpus_size)+' words in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to quickly clean text\n",
    "\n",
    "# import modules from nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "# define stop lists\n",
    "letters = list(string.ascii_lowercase)\n",
    "numbers = [str(i) for i in range(0, 1000)]\n",
    "boilerplate = ['000', '’s', '―', '/', 'playback', 'get', 'news', 'report', 'unsubscribe', 'they', 'must', 'share', 'that', 'view', 'hide', 'copy', 'something', 'enlarge', 'reprint', 'read', '_', 'videos', 'autoplay', 'watched', 'press', '’ve', 'toggle', 'around', 'the', 's.', 'said', 'here©', 'ad', '#', 'andhis', 'click', 'r', 'device', 'contributed', 'advertisement', 'the washington', '&', 'follow', 'copyright', 'mrs.', 'photo', 'to', 'also', 'times', 'for', 'however', 'fox', 'this', 'copyright ©', 'ofs', 'just', 'wait', 'n’t', 'told', 'unsupported', 'i', 'caption', 'ms.', '’m', 'paste', '’re', 'replay', 'photos', 'mr.', '©', 'skip', 'watch', '2018', 'cut', 'llc', 'more', 'post', 'embed', 'blog', 'b.', 'associated', 'permission']\n",
    "stop_list = set(stopwords.words('english') + boilerplate + numbers + letters)\n",
    "\n",
    "# create a tokenizer\n",
    "re_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "# simple preprocessing \n",
    "def clean(text):\n",
    "    \n",
    "    collection = []\n",
    "    \n",
    "    for i in range(0,len(text)):\n",
    "\n",
    "        article = text[i].lower()\n",
    "        \n",
    "        test_tokens = re_tokenizer.tokenize(article)\n",
    "\n",
    "        clean_tokens = []\n",
    "        for t in test_tokens:\n",
    "            if t not in stop_list:\n",
    "                clean_tokens.append(t)\n",
    "                \n",
    "        collection.append(clean_tokens)\n",
    "    \n",
    "    #clean_string = (' ').join(collection)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rewrite the dataframe such that: \n",
    "-clean text\n",
    "-option to convert training examples to paragraphs rather than documents\n",
    "\n",
    "'''\n",
    "def convert_text(option):\n",
    "    new_docset = []\n",
    "\n",
    "    # iterate through all the rows of the df\n",
    "    for i in range(0, num_rows):\n",
    "        # extract label and text information\n",
    "        label = new_df.iloc[i][0]\n",
    "        text = new_df.iloc[i][1]\n",
    "        \n",
    "        if option=='doc':\n",
    "            clean_text = ' '.join([' '.join(line) for line in clean(text.splitlines())])\n",
    "            new_docset.append([label, clean_text])\n",
    "            \n",
    "        elif option=='para':\n",
    "            # segment texts by paragraph and treat as individual documents\n",
    "            clean_text = [' '.join(line) for line in clean(text.splitlines())]\n",
    "            for line in clean_text:\n",
    "                new_docset.append([label, line])\n",
    "                \n",
    "    return new_docset\n",
    "\n",
    "doc_text = convert_text('doc')\n",
    "para_text = convert_text('para')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine properties of new corpus\n",
    "def corpus_info(corpus, name):\n",
    "    right_ct = 0\n",
    "    left_ct = 0\n",
    "    center_ct = 0\n",
    "    \n",
    "    for doc in corpus:\n",
    "        if doc[0] == 'Left':\n",
    "            left_ct +=1\n",
    "        elif doc[0] == 'Right':\n",
    "            right_ct +=1\n",
    "        else:\n",
    "            center_ct += 1\n",
    "    \n",
    "    print(name)\n",
    "    print('Size:', len(corpus))\n",
    "    print('Left:', '{0:.2f}'.format(left_ct/len(corpus)))\n",
    "    print('Right:', '{0:.2f}'.format(right_ct/len(corpus)))\n",
    "    print('Center:', '{0:.2f}'.format(center_ct/len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus\n",
      "Size: 3162\n",
      "Left: 0.42\n",
      "Right: 0.33\n",
      "Center: 0.26\n"
     ]
    }
   ],
   "source": [
    "corpus_info(doc_text, 'Original Corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph Corpus\n",
      "Size: 64309\n",
      "Left: 0.45\n",
      "Right: 0.28\n",
      "Center: 0.27\n"
     ]
    }
   ],
   "source": [
    "corpus_info(para_text, 'Paragraph Corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Size: 2532\n",
      "Left: 0.40\n",
      "Right: 0.33\n",
      "Center: 0.28\n",
      "Test\n",
      "Size: 630\n",
      "Left: 0.49\n",
      "Right: 0.34\n",
      "Center: 0.17\n"
     ]
    }
   ],
   "source": [
    "# shuffle corpus\n",
    "# from random import shuffle\n",
    "# shuffle(para_text)\n",
    "\n",
    "# partition into train and test sets\n",
    "#train = para_text[0:51447]\n",
    "#test = para_text[51447:64309]\n",
    "\n",
    "# partition into train and test sets\n",
    "train = doc_text[0:2532]\n",
    "test = doc_text[2532:3162]\n",
    "\n",
    "# evaluate training and test sets\n",
    "corpus_info(train, 'Train')\n",
    "corpus_info(test, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write corpus data to text file\n",
    "from os import path\n",
    "fastText_dir = path.join(path.dirname(\"__file__\"), 'fastText')\n",
    "\n",
    "def write_file(corpus, label):\n",
    "    f = open('fastText/'+label+'.txt', 'wt', encoding='utf-8')\n",
    "    for instance in corpus:\n",
    "        label = instance[0]\n",
    "        text = instance[1]\n",
    "        f.write('%s\\t%s\\n' % (label, text))\n",
    "\n",
    "write_file(para_text, 'para')\n",
    "write_file(doc_text, 'doc')\n",
    "# create training and test files\n",
    "#write_file(train, 'train')\n",
    "#write_file(test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "with open('fastText/doc.train.txt', \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"\\t\")\n",
    "        X.append(text.split())\n",
    "        y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get model results\n",
    "def test_model(model, test_data):\n",
    "    result = model.test(test_data)\n",
    "    print('Precision@1:', result[1])\n",
    "    print('Recall@1:', result[2])\n",
    "    print('Number of examples:', result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file information\n",
    "news_input = path.join(fastText_dir, 'train.txt')\n",
    "news_test = path.join(fastText_dir, 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fastText\n",
    "import fastText as ft\n",
    "\n",
    "# Info to save the model\n",
    "model_dir = path.join(path.dirname(\"__file__\"), 'models')\n",
    "news_output = path.join(model_dir, 'news')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model Parameters\n",
    "-lr                 learning rate [0.1]\n",
    "-dim                size of word vectors [100]\n",
    "-ws                 size of the context window [5]\n",
    "-epoch              number of epochs [5]\n",
    "-minCount           minimal number of word occurences [1]\n",
    "-minCountLabel      minimal number of label occurences [0]\n",
    "-minn               min length of char ngram [0]\n",
    "-maxn               max length of char ngram [0]\n",
    "-neg                number of negatives sampled [5]\n",
    "-wordNgrams         max length of word ngram [1]\n",
    "-loss               loss function {ns, hs, softmax} [softmax]\n",
    "-bucket             number of buckets [2000000]\n",
    "-thread             number of threads [12]\n",
    "-lrUpdateRate       change the rate of updates for the learning rate [100]\n",
    "-t                  sampling threshold [0.0001]\n",
    "-verbose            verbosity level [2]\n",
    "-pretrainedVectors  pretrained word vectors for supervised learning []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model = ft.train_supervised(news_input, lr=0.1, dim=100, ws=5, \n",
    "                              epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, \n",
    "                              neg=5, wordNgrams=1, loss='softmax', bucket=2000000, \n",
    "                              thread=12, lrUpdateRate=100, t=0.0001, label='__label__', \n",
    "                                 verbose=2, pretrainedVectors='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.4746031746031746\n",
      "Recall@1: 0.4746031746031746\n",
      "Number of examples: 630\n"
     ]
    }
   ],
   "source": [
    "test_model(news_model, news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def parse_labels(path):\n",
    "    with open(path) as f:\n",
    "        return np.array(list(map(lambda x: int(x[9:]), f.read().split())))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Display confusion matrix.')\n",
    "    parser.add_argument('test', help='Path to test labels')\n",
    "    parser.add_argument('predict', help='Path to predictions')\n",
    "    args = parser.parse_args()\n",
    "    test_labels = parse_labels(args.test)\n",
    "    pred_labels = parse_labels(args.predict)\n",
    "    eq = test_labels == pred_labels\n",
    "    print(\"Accuracy: \" + str(eq.sum() / len(test_labels)))\n",
    "    print(confusion_matrix(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fastText.FastText in fastText:\n",
      "\n",
      "NAME\n",
      "    fastText.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the BSD-style license found in the\n",
      "    # LICENSE file in the root directory of this source tree. An additional grant\n",
      "    # of patent rights can be found in the PATENTS file in the same directory.\n",
      "\n",
      "FUNCTIONS\n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(input, lr=0.1, dim=100, ws=5, epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, neg=5, wordNgrams=1, loss='softmax', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(input, model='skipgram', lr=0.05, dim=100, ws=5, epoch=5, minCount=5, minCountLabel=0, minn=3, maxn=6, neg=5, wordNgrams=1, loss='ns', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /Users/meldye/anaconda/envs/newscrawl/lib/python3.6/site-packages/fastText/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.FastText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
